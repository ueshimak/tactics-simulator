<<<<<<< HEAD
import time
import pandas as pd
import os
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import re

# âœ… è¨­å®š
EDGE_PATH = r"C:/Users/bxl07/AppData/Local/SeleniumBasic/edgedriver.exe"
URL_LIST_PAGE = "https://gamewith.jp/nobunagatenka/485928"
KAMIGAME_URL = "https://kamigame.jp/nobunagatenka/page/371287863528001044.html"
CSV_DIR = "csv_data"
DATA_CSV = os.path.join(CSV_DIR, "officers_fixed.csv")
NAME_CSV = os.path.join(CSV_DIR, "acquired_names.csv")

# âœ… ä¿å­˜ç”¨CSVã®åˆæœŸåŒ–
os.makedirs(CSV_DIR, exist_ok=True)
if not os.path.exists(DATA_CSV):
    pd.DataFrame(columns=[
        "æ­¦å°†å", "çµ±ç‡", "æ­¦å‹‡", "çŸ¥ç•¥", "æ”¿æ²»", "é€Ÿåº¦",
        "å›ºæœ‰æˆ¦æ³•å", "å›ºæœ‰æˆ¦æ³•åŠ¹æœ", "ãƒ¬ã‚¢ãƒªãƒ†ã‚£",
        "é¨é¦¬é©æ€§", "é‰„ç ²é©æ€§", "å¼“é©æ€§", "å…µå™¨é©æ€§", "è¶³è»½é©æ€§"
    ]).to_csv(DATA_CSV, index=False, encoding="utf-8-sig")

if not os.path.exists(NAME_CSV):
    pd.DataFrame(columns=["æ­¦å°†å"]).to_csv(NAME_CSV, index=False, encoding="utf-8-sig")

# âœ… ç¥ã‚²ãƒ¼æ”»ç•¥ã‹ã‚‰å…µç¨®ç‰¹æ€§ãƒãƒƒãƒ—å–å¾—
def getå…µç¨®é©æ€§ãƒãƒƒãƒ—():
    res = requests.get(KAMIGAME_URL)
    soup = BeautifulSoup(res.text, "lxml")
    rows = soup.select("#mainArticle > section > div:nth-child(1) > table > tbody > tr")

    ç‰¹æ€§ãƒãƒƒãƒ— = {}
    for row in rows:
        cols = row.select("td")
        if len(cols) < 4:
            continue
        name_tag = cols[0].select_one("span a")
        if not name_tag:
            continue
        name = name_tag.text.strip()

        é©æ€§_soup = BeautifulSoup(str(cols[3]), "lxml")
        text = é©æ€§_soup.get_text()

        matches = re.findall(r"ã€(.*?)ã€‘\s*([SABC])", text)
        ç‰¹æ€§ = {f"{å…µç¨®}é©æ€§": "A" for å…µç¨® in ["é¨é¦¬", "é‰„ç ²", "å¼“", "å…µå™¨", "è¶³è»½"]}

        for å…µç¨®å, ãƒ©ãƒ³ã‚¯ in matches:
            key = f"{å…µç¨®å.strip()}é©æ€§"
            if key in ç‰¹æ€§:
                ç‰¹æ€§[key] = ãƒ©ãƒ³ã‚¯

        ç‰¹æ€§ãƒãƒƒãƒ—[name] = ç‰¹æ€§

    return ç‰¹æ€§ãƒãƒƒãƒ—


å…µç¨®é©æ€§ãƒãƒƒãƒ— = getå…µç¨®é©æ€§ãƒãƒƒãƒ—()

# âœ… å–å¾—æ¸ˆã¿æ­¦å°†åãƒªã‚¹ãƒˆ
done_names = pd.read_csv(NAME_CSV)["æ­¦å°†å"].tolist()

# âœ… EdgeDriverèµ·å‹•
service = Service(executable_path=EDGE_PATH)
options = Options()
options.add_argument("user-agent=Mozilla/5.0")
options.add_argument("headless")
driver = webdriver.Edge(service=service, options=options)
wait = WebDriverWait(driver, 10)
driver.get(URL_LIST_PAGE)
time.sleep(5)

# âœ… æ­¦å°†ä¸€è¦§ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œå–å¾—
rows = driver.find_elements(By.CSS_SELECTOR, "#article-body .nobutenka_listtable table tbody tr")

for idx in range(1, len(rows)):
    try:
        link_selector = f"#article-body .nobutenka_listtable table tbody tr:nth-child({idx+1}) td:nth-child(1) a"
        link_element = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, link_selector)))
        name = link_element.text.strip()

        rarity_selector = f"#article-body .nobutenka_listtable table tbody tr:nth-child({idx+1}) td:nth-child(2) img"
        try:
            rarity_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, rarity_selector)))
            rarity = rarity_element.get_attribute("alt").strip()
        except:
            rarity = ""

        if name in done_names:
            print(f"â© æ—¢å–å¾—ï¼š{name}ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            continue

        print(f"\nğŸ§­ æ­¦å°†{idx}: {name}ï¼ˆ{rarity}ï¼‰ ãƒšãƒ¼ã‚¸é·ç§»")
        driver.execute_script("arguments[0].scrollIntoView(true);", link_element)
        driver.execute_script("arguments[0].click();", link_element)
        time.sleep(5)
        wait.until(EC.presence_of_element_located(
            (By.CSS_SELECTOR, "#article-body table")
        ))

        soup = BeautifulSoup(driver.page_source, "lxml")

        def extract_stat(rank_idx: int) -> str:
            tag = soup.select_one(f".gw_bar_table tr:nth-child({rank_idx}) .rank b")
            return tag.text.strip() if tag else ""

        leadership = extract_stat(1)
        bravery    = extract_stat(2)
        intellect  = extract_stat(3)
        politics   = extract_stat(4)
        speed      = extract_stat(5)

        # âœ… å›ºæœ‰æˆ¦æ³•åã¨åŠ¹æœæŠ½å‡º
        try:
            skill_name_tag = soup.select_one("#article-body > table:nth-child(21) > tbody > tr:nth-child(2) > td:nth-child(1)")
            skill_name = skill_name_tag.text.strip() if skill_name_tag else ""
        except:
            skill_name = ""

        try:
            effect_tag = soup.select_one("#article-body > table:nth-child(21) > tbody > tr:nth-child(2) > td:nth-child(2)")
            skill_effect = effect_tag.text.strip() if effect_tag else ""
        except:
            skill_effect = ""

        # âœ… å…µç¨®ç‰¹æ€§ãƒãƒƒãƒ—ã‹ã‚‰æŠ½å‡ºï¼ˆæœªç™»éŒ²ãªã‚‰ Aã§åˆæœŸåŒ–ï¼‰
        ç‰¹æ€§ = å…µç¨®é©æ€§ãƒãƒƒãƒ—.get(name, {f"{å…µç¨®}é©æ€§": "A" for å…µç¨® in ["é¨é¦¬", "é‰„ç ²", "å¼“", "å…µå™¨", "è¶³è»½"]})

        row = {
            "æ­¦å°†å": name,
            "çµ±ç‡": leadership,
            "æ­¦å‹‡": bravery,
            "çŸ¥ç•¥": intellect,
            "æ”¿æ²»": politics,
            "é€Ÿåº¦": speed,
            "å›ºæœ‰æˆ¦æ³•å": skill_name,
            "å›ºæœ‰æˆ¦æ³•åŠ¹æœ": skill_effect,
            "ãƒ¬ã‚¢ãƒªãƒ†ã‚£": rarity,
            **ç‰¹æ€§
        }

        pd.DataFrame([row]).to_csv(DATA_CSV, mode="a", header=False, index=False, encoding="utf-8-sig")
        pd.DataFrame([{"æ­¦å°†å": name}]).to_csv(NAME_CSV, mode="a", header=False, index=False, encoding="utf-8-sig")
        print(f"âœ… ä¿å­˜å®Œäº†ï¼š{name}ï¼ˆ{rarity}ï¼‰")

        driver.back()
        time.sleep(4)

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ï¼ˆæ­¦å°†{idx}ï¼‰: {str(e)}")
        try:
            driver.get(URL_LIST_PAGE)
            time.sleep(5)
        except:
            print("âš ï¸ ãƒšãƒ¼ã‚¸å†èª­ã¿è¾¼ã¿å¤±æ•—ã€å‡¦ç†ç¶šè¡Œ")
        continue

driver.quit()
=======
import time
import pandas as pd
import os
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import re

# âœ… è¨­å®š
EDGE_PATH = r"C:/Users/bxl07/AppData/Local/SeleniumBasic/edgedriver.exe"
URL_LIST_PAGE = "https://gamewith.jp/nobunagatenka/485928"
KAMIGAME_URL = "https://kamigame.jp/nobunagatenka/page/371287863528001044.html"
CSV_DIR = "csv_data"
DATA_CSV = os.path.join(CSV_DIR, "officers_fixed.csv")
NAME_CSV = os.path.join(CSV_DIR, "acquired_names.csv")

# âœ… ä¿å­˜ç”¨CSVã®åˆæœŸåŒ–
os.makedirs(CSV_DIR, exist_ok=True)
if not os.path.exists(DATA_CSV):
    pd.DataFrame(columns=[
        "æ­¦å°†å", "çµ±ç‡", "æ­¦å‹‡", "çŸ¥ç•¥", "æ”¿æ²»", "é€Ÿåº¦",
        "å›ºæœ‰æˆ¦æ³•å", "å›ºæœ‰æˆ¦æ³•åŠ¹æœ", "ãƒ¬ã‚¢ãƒªãƒ†ã‚£",
        "é¨é¦¬é©æ€§", "é‰„ç ²é©æ€§", "å¼“é©æ€§", "å…µå™¨é©æ€§", "è¶³è»½é©æ€§"
    ]).to_csv(DATA_CSV, index=False, encoding="utf-8-sig")

if not os.path.exists(NAME_CSV):
    pd.DataFrame(columns=["æ­¦å°†å"]).to_csv(NAME_CSV, index=False, encoding="utf-8-sig")

# âœ… ç¥ã‚²ãƒ¼æ”»ç•¥ã‹ã‚‰å…µç¨®ç‰¹æ€§ãƒãƒƒãƒ—å–å¾—
def getå…µç¨®é©æ€§ãƒãƒƒãƒ—():
    res = requests.get(KAMIGAME_URL)
    soup = BeautifulSoup(res.text, "lxml")
    rows = soup.select("#mainArticle > section > div:nth-child(1) > table > tbody > tr")

    ç‰¹æ€§ãƒãƒƒãƒ— = {}
    for row in rows:
        cols = row.select("td")
        if len(cols) < 4:
            continue
        name_tag = cols[0].select_one("span a")
        if not name_tag:
            continue
        name = name_tag.text.strip()

        é©æ€§_soup = BeautifulSoup(str(cols[3]), "lxml")
        text = é©æ€§_soup.get_text()

        matches = re.findall(r"ã€(.*?)ã€‘\s*([SABC])", text)
        ç‰¹æ€§ = {f"{å…µç¨®}é©æ€§": "A" for å…µç¨® in ["é¨é¦¬", "é‰„ç ²", "å¼“", "å…µå™¨", "è¶³è»½"]}

        for å…µç¨®å, ãƒ©ãƒ³ã‚¯ in matches:
            key = f"{å…µç¨®å.strip()}é©æ€§"
            if key in ç‰¹æ€§:
                ç‰¹æ€§[key] = ãƒ©ãƒ³ã‚¯

        ç‰¹æ€§ãƒãƒƒãƒ—[name] = ç‰¹æ€§

    return ç‰¹æ€§ãƒãƒƒãƒ—


å…µç¨®é©æ€§ãƒãƒƒãƒ— = getå…µç¨®é©æ€§ãƒãƒƒãƒ—()

# âœ… å–å¾—æ¸ˆã¿æ­¦å°†åãƒªã‚¹ãƒˆ
done_names = pd.read_csv(NAME_CSV)["æ­¦å°†å"].tolist()

# âœ… EdgeDriverèµ·å‹•
service = Service(executable_path=EDGE_PATH)
options = Options()
options.add_argument("user-agent=Mozilla/5.0")
options.add_argument("headless")
driver = webdriver.Edge(service=service, options=options)
wait = WebDriverWait(driver, 10)
driver.get(URL_LIST_PAGE)
time.sleep(5)

# âœ… æ­¦å°†ä¸€è¦§ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œå–å¾—
rows = driver.find_elements(By.CSS_SELECTOR, "#article-body .nobutenka_listtable table tbody tr")

for idx in range(1, len(rows)):
    try:
        link_selector = f"#article-body .nobutenka_listtable table tbody tr:nth-child({idx+1}) td:nth-child(1) a"
        link_element = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, link_selector)))
        name = link_element.text.strip()

        rarity_selector = f"#article-body .nobutenka_listtable table tbody tr:nth-child({idx+1}) td:nth-child(2) img"
        try:
            rarity_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, rarity_selector)))
            rarity = rarity_element.get_attribute("alt").strip()
        except:
            rarity = ""

        if name in done_names:
            print(f"â© æ—¢å–å¾—ï¼š{name}ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            continue

        print(f"\nğŸ§­ æ­¦å°†{idx}: {name}ï¼ˆ{rarity}ï¼‰ ãƒšãƒ¼ã‚¸é·ç§»")
        driver.execute_script("arguments[0].scrollIntoView(true);", link_element)
        driver.execute_script("arguments[0].click();", link_element)
        time.sleep(5)
        wait.until(EC.presence_of_element_located(
            (By.CSS_SELECTOR, "#article-body table")
        ))

        soup = BeautifulSoup(driver.page_source, "lxml")

        def extract_stat(rank_idx: int) -> str:
            tag = soup.select_one(f".gw_bar_table tr:nth-child({rank_idx}) .rank b")
            return tag.text.strip() if tag else ""

        leadership = extract_stat(1)
        bravery    = extract_stat(2)
        intellect  = extract_stat(3)
        politics   = extract_stat(4)
        speed      = extract_stat(5)

        # âœ… å›ºæœ‰æˆ¦æ³•åã¨åŠ¹æœæŠ½å‡º
        try:
            skill_name_tag = soup.select_one("#article-body > table:nth-child(21) > tbody > tr:nth-child(2) > td:nth-child(1)")
            skill_name = skill_name_tag.text.strip() if skill_name_tag else ""
        except:
            skill_name = ""

        try:
            effect_tag = soup.select_one("#article-body > table:nth-child(21) > tbody > tr:nth-child(2) > td:nth-child(2)")
            skill_effect = effect_tag.text.strip() if effect_tag else ""
        except:
            skill_effect = ""

        # âœ… å…µç¨®ç‰¹æ€§ãƒãƒƒãƒ—ã‹ã‚‰æŠ½å‡ºï¼ˆæœªç™»éŒ²ãªã‚‰ Aã§åˆæœŸåŒ–ï¼‰
        ç‰¹æ€§ = å…µç¨®é©æ€§ãƒãƒƒãƒ—.get(name, {f"{å…µç¨®}é©æ€§": "A" for å…µç¨® in ["é¨é¦¬", "é‰„ç ²", "å¼“", "å…µå™¨", "è¶³è»½"]})

        row = {
            "æ­¦å°†å": name,
            "çµ±ç‡": leadership,
            "æ­¦å‹‡": bravery,
            "çŸ¥ç•¥": intellect,
            "æ”¿æ²»": politics,
            "é€Ÿåº¦": speed,
            "å›ºæœ‰æˆ¦æ³•å": skill_name,
            "å›ºæœ‰æˆ¦æ³•åŠ¹æœ": skill_effect,
            "ãƒ¬ã‚¢ãƒªãƒ†ã‚£": rarity,
            **ç‰¹æ€§
        }

        pd.DataFrame([row]).to_csv(DATA_CSV, mode="a", header=False, index=False, encoding="utf-8-sig")
        pd.DataFrame([{"æ­¦å°†å": name}]).to_csv(NAME_CSV, mode="a", header=False, index=False, encoding="utf-8-sig")
        print(f"âœ… ä¿å­˜å®Œäº†ï¼š{name}ï¼ˆ{rarity}ï¼‰")

        driver.back()
        time.sleep(4)

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ï¼ˆæ­¦å°†{idx}ï¼‰: {str(e)}")
        try:
            driver.get(URL_LIST_PAGE)
            time.sleep(5)
        except:
            print("âš ï¸ ãƒšãƒ¼ã‚¸å†èª­ã¿è¾¼ã¿å¤±æ•—ã€å‡¦ç†ç¶šè¡Œ")
        continue

driver.quit()
>>>>>>> 6cc6e22068fd297841f9570f83ea405279f90609
print("\nğŸ“¦ å…¨å‡¦ç†å®Œäº†ã€‚ãƒ‡ãƒ¼ã‚¿ä¿å­˜æ¸ˆã¿ã€‚")